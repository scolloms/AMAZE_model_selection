{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyCBC.libutils: pkg-config call failed, setting NO_PKGCONFIG=1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "glasflow is using its own internal version of nflows\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pylab import rcParams\n",
    "import h5py\n",
    "import seaborn as sns\n",
    "from scipy.special import logit\n",
    "from scipy.special import expit\n",
    "from scipy.interpolate import RegularGridInterpolator\n",
    "from scipy.stats import entropy\n",
    "from pylab import rcParams\n",
    "import sys\n",
    "import torch\n",
    "import scipy\n",
    "import corner\n",
    "import copy\n",
    "\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "from populations.bbh_models import read_hdf5\n",
    "import populations.bbh_models as read_models\n",
    "from populations.Flowsclass_dev import FlowModel\n",
    "from populations.utils.bounded_Nd_kde import Bounded_Nd_kde\n",
    "from populations import gw_obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "param = ['mchirp','q', 'chieff', 'z']\n",
    "no_params = len(param)\n",
    "channel_label = 'CE'\n",
    "chi_b = [0.0,0.1,0.2,0.5]\n",
    "alpha_CE = [0.2,0.5,1.0,2.,5.]\n",
    "\n",
    "channel_ids = {'CE':0, 'CHE':1,'GC':2,'NSC':3, 'SMT':4}\n",
    "channel_id = channel_ids[channel_label] #will be 0, 1, 2, 3, or 4\n",
    "channel_samples = [4e6,864124,896611,582961, 4e6]\n",
    "no_binaries = int(channel_samples[channel_id])\n",
    "\n",
    "models_path ='/Users/stormcolloms/Documents/PhD/Project_work/OneChannel_Flows/models_reduced.hdf5'\n",
    "popsynth_outputs = read_hdf5(models_path, channel_label) # read all data from hdf5 file\n",
    "\n",
    "models_dict = dict.fromkeys(popsynth_outputs.keys())\n",
    "weights_dict = dict.fromkeys(popsynth_outputs.keys())\n",
    "\n",
    "\n",
    "for key in popsynth_outputs.keys():\n",
    "    models_dict[key] = popsynth_outputs[key][param]\n",
    "    weights_dict[key]= popsynth_outputs[key]['weight']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:03<00:00,  3.91s/it]\n",
      "100%|██████████| 20/20 [00:08<00:00,  2.34it/s]\n"
     ]
    }
   ],
   "source": [
    "params = ['mchirp','q', 'chieff', 'z']\n",
    "file_path='/Users/stormcolloms/Documents/PhD/Project_work/OneChannel_Flows/models_reduced.hdf5'\n",
    "gw_path = '/Users/stormcolloms/Documents/PhD/Project_work/AMAZE_model_selection/gw_events'\n",
    "observations, obsdata, p_theta, events = gw_obs.generate_observations(params, gw_path, \\\n",
    "                                            100, 'posteriors', None)\n",
    "\n",
    "model_names, flow = read_models.get_models(file_path, [channel_label], params, use_flows=True, device='cpu', no_bins=[5], use_unityweights=False)\n",
    "_, KDE = read_models.get_models(file_path, [channel_label], params, use_flows=False, device='cpu', use_unityweights=False)\n",
    "\n",
    "#inputs: x, data, pop_models, submodels_dict, channels, use_flows\n",
    "hyperparams = list(set([x.split('/', 1)[1] for x in model_names]))\n",
    "Nhyper = np.max([len(x.split('/')) for x in hyperparams])\n",
    "channels = sorted(list(set([x.split('/')[0] for x in model_names])))\n",
    "\n",
    "# construct dict that relates submodels to their index number\n",
    "submodels_dict = {} #dummy index dict keys:0,1,2,3, items: particular models\n",
    "ctr=0 #associates with either chi_b or alpha (0 or 1)\n",
    "while ctr < Nhyper:\n",
    "    submodels_dict[ctr] = {}\n",
    "    hyper_set = sorted(list(set([x.split('/')[ctr] for x in hyperparams])))\n",
    "    for idx, model in enumerate(hyper_set): #idx associates with 0,1,2,3,(4) keys\n",
    "        submodels_dict[ctr][idx] = model\n",
    "    ctr += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "flow_path=\"/Users/stormcolloms/Documents/PhD/Project_work/AMAZE_model_selection/rns/Flows_150124_extralong/flow_models/\"\n",
    "flow[channel_label].load_model(flow_path, channel_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(92)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3134718516310004\n",
      "1.5754119777909987\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [14], line 9\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[39mfor\u001b[39;00m alpha_id, a \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(alpha_CE):\n\u001b[1;32m      7\u001b[0m     flow_samples_stack \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mexp(flow[channel_label](obsdata, np\u001b[39m.\u001b[39marray([chi_b_id,alpha_id]), \u001b[39m9909\u001b[39m, p_theta))\n\u001b[0;32m----> 9\u001b[0m     kde_samples \u001b[39m=\u001b[39m KDE[channel_label][submodels_dict[\u001b[39m0\u001b[39;49m][\u001b[39m0\u001b[39;49m]][submodels_dict[\u001b[39m1\u001b[39;49m][\u001b[39m0\u001b[39;49m]](obsdata, \u001b[39m9909\u001b[39;49m, p_theta)\n\u001b[1;32m     11\u001b[0m     model_samples_idx \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mrandom\u001b[39m.\u001b[39mchoice(np\u001b[39m.\u001b[39mshape(models_dict[(chi_b_id,alpha_id)])[\u001b[39m0\u001b[39m], no_samples, \\\n\u001b[1;32m     12\u001b[0m         p\u001b[39m=\u001b[39mweights_dict[(chi_b_id,alpha_id)]\u001b[39m/\u001b[39mnp\u001b[39m.\u001b[39msum(weights_dict[(chi_b_id,alpha_id)]))\n\u001b[1;32m     13\u001b[0m     model_samples \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray(models_dict[(chi_b_id,alpha_id)])[model_samples_idx]\n",
      "File \u001b[0;32m~/Documents/PhD/Project_work/AMAZE_model_selection/testing_notebooks/../populations/__init__.py:271\u001b[0m, in \u001b[0;36mKDEModel.__call__\u001b[0;34m(self, data, smallest_N, data_pdf, proc_idx, return_dict)\u001b[0m\n\u001b[1;32m    268\u001b[0m data_pdf[data_pdf\u001b[39m==\u001b[39m\u001b[39m0\u001b[39m] \u001b[39m=\u001b[39m \u001b[39m1e-50\u001b[39m\n\u001b[1;32m    269\u001b[0m \u001b[39mfor\u001b[39;00m idx, (obs, d_pdf) \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(\u001b[39mzip\u001b[39m(np\u001b[39m.\u001b[39matleast_3d(data),data_pdf)):\n\u001b[1;32m    270\u001b[0m     \u001b[39m# Evaluate the KDE at the samples\u001b[39;00m\n\u001b[0;32m--> 271\u001b[0m     likelihood_per_samp \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpdf(obs) \u001b[39m/\u001b[39m d_pdf\n\u001b[1;32m    272\u001b[0m     likelihood[idx] \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m (\u001b[39m1.0\u001b[39m\u001b[39m/\u001b[39m\u001b[39mlen\u001b[39m(obs)) \u001b[39m*\u001b[39m np\u001b[39m.\u001b[39msum(likelihood_per_samp)\n\u001b[1;32m    273\u001b[0m \u001b[39m# store value for multiprocessing\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/PhD/Project_work/AMAZE_model_selection/testing_notebooks/../populations/__init__.py:185\u001b[0m, in \u001b[0;36mKDEModel.__init__.<locals>.<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    183\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    184\u001b[0m     kde \u001b[39m=\u001b[39m Bounded_Nd_kde(samples\u001b[39m.\u001b[39mT, weights\u001b[39m=\u001b[39mcombined_weights, bw_method\u001b[39m=\u001b[39mbandwidth, bounds\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mparam_bounds)\n\u001b[0;32m--> 185\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpdf \u001b[39m=\u001b[39m  \u001b[39mlambda\u001b[39;00m x: kde(x\u001b[39m.\u001b[39;49mT)\n\u001b[1;32m    186\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mkde \u001b[39m=\u001b[39m kde\n\u001b[1;32m    188\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcached_values \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/PhD/Project_work/AMAZE_model_selection/testing_notebooks/../populations/utils/bounded_Nd_kde.py:164\u001b[0m, in \u001b[0;36mBounded_Nd_kde.__call__\u001b[0;34m(self, pts)\u001b[0m\n\u001b[1;32m    161\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbounds[dim,\u001b[39m1\u001b[39m] \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    162\u001b[0m             out_of_bounds[pts[dim, :] \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbounds[dim,\u001b[39m1\u001b[39m]] \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m--> 164\u001b[0m results, norm_fac \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mevaluate(pts)\n\u001b[1;32m    166\u001b[0m \u001b[39m# set out-of-bound evaluations to 0\u001b[39;00m\n\u001b[1;32m    167\u001b[0m results[out_of_bounds] \u001b[39m=\u001b[39m \u001b[39m0.0\u001b[39m\n",
      "File \u001b[0;32m~/Documents/PhD/Project_work/AMAZE_model_selection/testing_notebooks/../populations/utils/bounded_Nd_kde.py:113\u001b[0m, in \u001b[0;36mBounded_Nd_kde.evaluate\u001b[0;34m(self, eval_pts)\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[39massert\u001b[39;00m eval_pts\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m] \u001b[39m==\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mNdim, \u001b[39m'\u001b[39m\u001b[39mevaluation points must be same number of dimensions as kde\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m    112\u001b[0m \u001b[39m# inherit original instance of kde and evaluate points\u001b[39;00m\n\u001b[0;32m--> 113\u001b[0m pdf \u001b[39m=\u001b[39m \u001b[39msuper\u001b[39;49m(Bounded_Nd_kde, \u001b[39mself\u001b[39;49m)\u001b[39m.\u001b[39;49mevaluate(eval_pts)\n\u001b[1;32m    115\u001b[0m \u001b[39m# return extra normalization factor for the reflections\u001b[39;00m\n\u001b[1;32m    116\u001b[0m norm_fac \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_refl_norm_factor\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/amaze/lib/python3.9/site-packages/scipy/stats/_kde.py:253\u001b[0m, in \u001b[0;36mgaussian_kde.evaluate\u001b[0;34m(self, points)\u001b[0m\n\u001b[1;32m    250\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    251\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m has unexpected item size \u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m'\u001b[39m \u001b[39m%\u001b[39m\n\u001b[1;32m    252\u001b[0m                     (output_dtype, itemsize))\n\u001b[0;32m--> 253\u001b[0m result \u001b[39m=\u001b[39m gaussian_kernel_estimate[spec](\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset\u001b[39m.\u001b[39;49mT, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweights[:, \u001b[39mNone\u001b[39;49;00m],\n\u001b[1;32m    254\u001b[0m                                         points\u001b[39m.\u001b[39;49mT, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minv_cov, output_dtype)\n\u001b[1;32m    255\u001b[0m \u001b[39mreturn\u001b[39;00m result[:, \u001b[39m0\u001b[39m]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "no_samples =100000\n",
    "flow_KDE_KL = np.zeros((4,5))\n",
    "\n",
    "for chi_b_id, xb in enumerate(chi_b):\n",
    "    for alpha_id, a in enumerate(alpha_CE):\n",
    "\n",
    "        flow_samples_stack = np.exp(flow[channel_label](obsdata, np.array([chi_b_id,alpha_id]), 9909, p_theta))\n",
    "\n",
    "        kde_samples = KDE[channel_label][submodels_dict[0][0]][submodels_dict[1][0]](obsdata, 9909, p_theta)\n",
    "\n",
    "        model_samples_idx = np.random.choice(np.shape(models_dict[(chi_b_id,alpha_id)])[0], no_samples, \\\n",
    "            p=weights_dict[(chi_b_id,alpha_id)]/np.sum(weights_dict[(chi_b_id,alpha_id)]))\n",
    "        model_samples = np.array(models_dict[(chi_b_id,alpha_id)])[model_samples_idx]\n",
    "\n",
    "        flow_KDE_KL[chi_b_id,alpha_id] = entropy(flow_samples_stack, kde_samples)\n",
    "        print(flow_KDE_KL[chi_b_id,alpha_id])\n",
    "\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.15 ('amaze')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "40f4eedd3c2b0a4cf443053300595e4c410b663f293e84798d1742802fb2c7c8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
