{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving Samples [mchirp,q,chieff,z] from Normalising flow models to HDF5 file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyCBC.libutils: pkg-config call failed, setting NO_PKGCONFIG=1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "glasflow is using its own internal version of nflows\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import corner as corner\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import h5py\n",
    "import torch\n",
    "import time\n",
    "from scipy.special import logsumexp\n",
    "\n",
    "import sys\n",
    "\n",
    "sys.path.append('../')\n",
    "from populations.bbh_models import get_models\n",
    "import populations.bbh_models as read_models\n",
    "from populations.utils.flow import NFlow\n",
    "from populations.Flowsclass_dev import FlowModel\n",
    "from populations import gw_obs\n",
    "\n",
    "from sample import sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 36/36 [00:18<00:00,  1.90it/s]\n"
     ]
    }
   ],
   "source": [
    "params = ['mchirp','q', 'chieff', 'z']\n",
    "channels = ['CE','CHE','GC','NSC','SMT']\n",
    "chi_b = [0.0,0.1,0.2,0.5]\n",
    "alpha = [0.2,0.5,1.,2.,5.]\n",
    "file_path='/Users/stormcolloms/Documents/PhD/Project_work/OneChannel_Flows/models_reduced.hdf5'\n",
    "\n",
    "model_names, KDEs = read_models.get_models(file_path, channels, params, use_flows=False, device='cpu')\n",
    "model_names.sort()\n",
    "hyperparams = sorted(list(set([x.split('/', 1)[1] for x in model_names])))\n",
    "Nhyper = np.max([len(x.split('/')) for x in hyperparams])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparam_dict  = {}\n",
    "hyperidx=0\n",
    "while hyperidx < Nhyper:\n",
    "    hyperidx_with_Nhyper = np.argwhere(np.asarray([len(x.split('/')) for x in hyperparams])>hyperidx).flatten()\n",
    "    hyperparams_at_level = sorted(set([x.split('/')[hyperidx] for x in np.asarray(hyperparams)[hyperidx_with_Nhyper]]))\n",
    "    hyperparam_dict[hyperidx] = hyperparams_at_level\n",
    "    hyperidx += 1\n",
    "all_models_at_deepest = all([len(x.split('/')[1:])==Nhyper for x in model_names])\n",
    "while all_models_at_deepest==False:\n",
    "    # loop until all models have the same length\n",
    "    for model in model_names:\n",
    "        # See number of hyperparameters in model, subtract one for channel\n",
    "        Nhyper_in_model = len(model.split('/'))-1\n",
    "        # loop until this model has all the hyperparam levels as well\n",
    "        while Nhyper_in_model < Nhyper:\n",
    "            model_names.remove(model)\n",
    "            for new_hyperparam in hyperparam_dict[Nhyper_in_model]:\n",
    "                # add new model name\n",
    "                model_names.append(model+'/'+new_hyperparam)\n",
    "            Nhyper_in_model += 1\n",
    "        model_names.sort()\n",
    "    # see if all models are at deepest level else repeat\n",
    "    all_models_at_deepest = all([len(x.split('/')[1:])==Nhyper for x in model_names])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#inputs: x, data, pop_models, submodels_dict, channels, use_flows\n",
    "hyperparams = list(set([x.split('/', 1)[1] for x in model_names]))\n",
    "Nhyper = np.max([len(x.split('/')) for x in hyperparams])\n",
    "channels = sorted(list(set([x.split('/')[0] for x in model_names])))\n",
    "\n",
    "# construct dict that relates submodels to their index number\n",
    "submodels_dict = {} #dummy index dict keys:0,1,2,3, items: particular models\n",
    "ctr=0 #associates with either chi_b or alpha (0 or 1)\n",
    "while ctr < Nhyper:\n",
    "    submodels_dict[ctr] = {}\n",
    "    hyper_set = sorted(list(set([x.split('/')[ctr] for x in hyperparams])))\n",
    "    for idx, model in enumerate(hyper_set): #idx associates with 0,1,2,3,(4) keys\n",
    "        submodels_dict[ctr][idx] = model\n",
    "    ctr += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: {0: 'chi00', 1: 'chi01', 2: 'chi02', 3: 'chi05'},\n",
       " 1: {0: 'alpha02', 1: 'alpha05', 2: 'alpha10', 3: 'alpha20', 4: 'alpha50'}}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submodels_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_models = []\n",
    "models = []\n",
    "def find_submodels(name, obj):\n",
    "        if isinstance(obj, h5py.Dataset):\n",
    "            all_models.append(name.rsplit('/', 1)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = h5py.File(file_path, 'r')\n",
    "f.visititems(find_submodels)\n",
    "# get all unique models\n",
    "all_models = sorted(list(set(all_models)))\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'alpha05'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submodels_dict[1][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "for chnl in channels:\n",
    "    for chibid in range(4):\n",
    "        for alphaid in range(5):\n",
    "            if chnl == 'CE':\n",
    "                samples = KDEs[chnl][submodels_dict[0][chibid]][submodels_dict[1][chibid]].sample(1000000)\n",
    "                df = pd.DataFrame(data=samples, columns=params)\n",
    "                df.to_hdf(\"KDE_samples.hdf5\", key=f'{chnl}/{submodels_dict[0][chibid]}/{submodels_dict[1][alphaid]}')\n",
    "            else:\n",
    "                samples = KDEs[chnl][submodels_dict[0][chibid]].sample(1000000)\n",
    "                df = pd.DataFrame(data=samples, columns=params)\n",
    "                df.to_hdf(\"KDE_samples.hdf5\", key=f'{chnl}/{submodels_dict[0][chibid]}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.15 ('amaze')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "40f4eedd3c2b0a4cf443053300595e4c410b663f293e84798d1742802fb2c7c8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
